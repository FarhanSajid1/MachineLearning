{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarhanSajid1/MachineLearning/blob/master/Stock_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "s8d5VWWZ9uwn",
        "colab_type": "code",
        "outputId": "ae3cefcf-3551-4f84-b8de-be604dab70ca",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ef86385-38d8-4fc2-bf0e-7400aeebb7cd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0ef86385-38d8-4fc2-bf0e-7400aeebb7cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving twitter.csv to twitter.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yt0MbhHr-BUT",
        "colab_type": "code",
        "outputId": "206e906f-faad-4c2e-d06c-14445e9abff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('twitter.csv')\n",
        "print(len(df))\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "            created_at                                               text  \\\n",
            "0  2013-01-01 20:10:04  Kickers on my watchlist $XIDE $TRIT $SOQ $PNK ...   \n",
            "1  2013-01-01 20:33:37  \"@user: $AAPL MOVIE. 55% return for the FEAR/G...   \n",
            "2  2013-01-01 21:43:41  @user I'd be afraid to short $AMZN - they are ...   \n",
            "3  2013-01-02 01:49:48                             $MNTA Over $12.00 URL    \n",
            "4  2013-01-02 01:51:33                              $OI  Over $21.37 URL    \n",
            "\n",
            "  sentiment  \n",
            "0  positive  \n",
            "1  positive  \n",
            "2  positive  \n",
            "3  positive  \n",
            "4  positive  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 3 columns):\n",
            "created_at    5000 non-null object\n",
            "text          5000 non-null object\n",
            "sentiment     5000 non-null object\n",
            "dtypes: object(3)\n",
            "memory usage: 117.3+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTLBlzoK-MyL",
        "colab_type": "code",
        "outputId": "8bda1de5-c523-4887-9ee8-78b309fbf30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(df.loc[df['sentiment'] == 'positive'])) #this returns the amount of positive samples we have\n",
        "print(len(df.loc[df['sentiment'] == 'negative'])) #this returns the amount of negative samples that we havve\n",
        "print(3350 + 1650)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3350\n",
            "1650\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cs2dme_G_JoE",
        "colab_type": "code",
        "outputId": "23e97f2f-50d5-4273-be63-59e1a673acfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Create numpy arrays with the values that we need. \n",
        "text = df['text'].values\n",
        "labels = df['sentiment'].values\n",
        "print(text.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pqo_Erpw_bWe",
        "colab_type": "code",
        "outputId": "46eb8c15-668d-4fd1-ca42-de60810851d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# preprocess the data so that we can read it in the RNN\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "#figuring out how many words should be in each padding sequence\n",
        "len_tokens=[len(token) for token in text]\n",
        "mean_value = np.mean(len_tokens) # 76.40 is the mean\n",
        "print(np.max(len_tokens)) # 140\n",
        "print(mean_value) # 76.40\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "140\n",
            "76.4056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrjV3OwZApM6",
        "colab_type": "code",
        "outputId": "10078393-0ad7-49e8-bba3-0d5808a26a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# creating testing and validation sampling\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "max_len = int(mean_value + 1* np.std(len_tokens)) # 112 l\n",
        "training = int(len((text)) * .8) # 4000\n",
        "testing = int(len((text)) * .2) # 1000\n",
        "\n",
        "max_words = 10000 # maximum words in our universal vocabulary\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "#fit the tokenizer on the text\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "#padding the data\n",
        "data = pad_sequences(sequences, maxlen = max_len)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "\n",
        "#shuffling and splitting the data!\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "X_train = data[: training]\n",
        "X_test = data[training:]\n",
        "\n",
        "y_train = labels[:training]\n",
        "y_test = labels[training:]\n",
        "\n",
        "#we need to label binarize the data\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.fit_transform(y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape) #this is for the sigmoid output binary\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 112)\n",
            "(1000, 112)\n",
            "(4000, 1)\n",
            "(1000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pZcX1-6xCrVQ",
        "colab_type": "code",
        "outputId": "6b88d8a1-f19e-4059-d0dc-12bd7f186acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "print(data[0]) #training sequence after the padding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0   12 1167  190   28 4327  108   81   32 1130\n",
            " 4328   79    4  589  409    2  144   10   43   46  547  167   51   38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DCLITSN5Cu7w",
        "colab_type": "code",
        "outputId": "223f1c17-5558-4077-eab6-8833e69ab35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kickers on my watchlist $XIDE $TRIT $SOQ $PNK $CPWR $BPZ $ALJ  trade method 1 or method 2, see prev posts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iBzhjXs3DLod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "reverse_word_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1XHgs5eYDYMo",
        "colab_type": "code",
        "outputId": "3fdba73f-dab8-40bd-ba77-f757fd8eb83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras import layers\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import LSTM, GRU\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 128))\n",
        "# model.add(layers.(256,return_sequences=True, recurrent_dropout = .1, kernel_regularizer = regularizers.l2(0.001)))\n",
        "# model.add(layers.Dropout(.1))\n",
        "# model.add(layers.GRU(128,return_sequences=True, recurrent_dropout = .2, kernel_regularizer = regularizers.l2(0.001)))\n",
        "# model.add(layers.Dropout(.2))\n",
        "# model.add(layers.GRU(64,return_sequences=True, recurrent_dropout = .1, kernel_regularizer = regularizers.l2(0.001)))\n",
        "# model.add(layers.Dropout(.2))\n",
        "# model.add(layers.GRU(32, kernel_regularizer = regularizers.l2(0.001)))\n",
        "# model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.add(Bidirectional(GRU(128, return_sequences = True,  kernel_regularizer = regularizers.l2(0.001), recurrent_dropout = .2)))\n",
        "model.add(layers.Dropout(.2))\n",
        "model.add(Bidirectional(GRU(64, return_sequences = True,  kernel_regularizer = regularizers.l2(0.1))))\n",
        "model.add(Bidirectional(GRU(32,  kernel_regularizer = regularizers.l2(0.1))))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "history = model.fit(X_train, y_train, epochs = 8, batch_size = 128, validation_data = [X_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/8\n",
            "4000/4000 [==============================] - 76s 19ms/step - loss: 40.3196 - acc: 0.6680 - val_loss: 23.9190 - val_acc: 0.7040\n",
            "Epoch 2/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 15.6305 - acc: 0.7935 - val_loss: 8.6860 - val_acc: 0.7720\n",
            "Epoch 3/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 5.0835 - acc: 0.8708 - val_loss: 2.5653 - val_acc: 0.7520\n",
            "Epoch 4/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 1.3245 - acc: 0.9125 - val_loss: 1.0160 - val_acc: 0.7840\n",
            "Epoch 5/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 0.3938 - acc: 0.9360 - val_loss: 1.0282 - val_acc: 0.7520\n",
            "Epoch 6/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 0.2305 - acc: 0.9483 - val_loss: 0.6351 - val_acc: 0.7710\n",
            "Epoch 7/8\n",
            "4000/4000 [==============================] - 71s 18ms/step - loss: 0.1925 - acc: 0.9560 - val_loss: 0.5396 - val_acc: 0.7920\n",
            "Epoch 8/8\n",
            "4000/4000 [==============================] - 72s 18ms/step - loss: 0.3051 - acc: 0.9285 - val_loss: 0.8481 - val_acc: 0.7510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebHo85tdhA_O",
        "colab_type": "code",
        "outputId": "ab6f77a8-3765-4223-a12a-43c269648222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#evaluating the accuracy of the network\n",
        "\n",
        "result = model.evaluate(X_test, y_test)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 6s 6ms/step\n",
            "[0.848092288017273, 0.751]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "109sGLOnhhGK",
        "colab_type": "code",
        "outputId": "7db8b6ea-f200-4c0c-9561-b12a6426de14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# analyzing the misclassified text\n",
        "\n",
        "y_predict = model.predict(x=X_test[0:100]) #testing on the first 100\n",
        "y_predict = y_predict\n",
        "y_predict.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "lNmxVQCyh0UU",
        "colab_type": "code",
        "outputId": "ea7660f8-6844-45c8-93fd-74e3d5851634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clas_pred = np.array([1 if p >.5 else 0 for p in y_predict])\n",
        "# this is a one liner which will convert anything that is greater than .5 to a 1, making it fire a 1, and anything lower as a 0\n",
        "#this way it makes classification easier. As opposed to numbers between 0-1 we have values 0 or 1\n",
        "#finally the last reason why we do this is so that we can compare these values to the values that we expect to be in the classificaiton\n",
        "true_values = np.array(y_test[0:100]) #these are the true labels\n",
        "incorrect = np.where(clas_pred != true_values)\n",
        "true_values.shape\n",
        "incorrect\n",
        "# print(incorrect[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  0,  0, ..., 99, 99, 99]), array([ 3,  6,  7, ..., 93, 97, 98]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "unlMcG12ky-Q",
        "colab_type": "code",
        "outputId": "4b2d73a5-a69b-4e31-f25d-1fa531e709b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A3DLPxd2iiey",
        "colab_type": "code",
        "outputId": "0d96a5e6-7f08-43ee-d5d9-c27babf4de01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "idices = incorrect[0]\n",
        "idices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0, ..., 99, 99, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "4CQAQaMMkUgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "    \n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSeFw2hjkI3J",
        "colab_type": "code",
        "outputId": "b4adedfc-cd67-4143-c0d5-a79104909dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(text[idices])\n",
        "print(labels[idices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Kickers on my watchlist $XIDE $TRIT $SOQ $PNK $CPWR $BPZ $ALJ  trade method 1 or method 2, see prev posts'\n",
            " 'Kickers on my watchlist $XIDE $TRIT $SOQ $PNK $CPWR $BPZ $ALJ  trade method 1 or method 2, see prev posts'\n",
            " 'Kickers on my watchlist $XIDE $TRIT $SOQ $PNK $CPWR $BPZ $ALJ  trade method 1 or method 2, see prev posts'\n",
            " ... '$VRNG buys vs. Sells?' '$VRNG buys vs. Sells?'\n",
            " '$VRNG buys vs. Sells?']\n",
            "['negative' 'negative' 'negative' ... 'positive' 'positive' 'positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsOSql3XmjN0",
        "colab_type": "code",
        "outputId": "82d426ad-e4d2-4321-f683-7d1197c84be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# this is testing some random text to see if it is positve or negative. \n",
        "# text1 = '$APPL is definitely going to run. I expect it to gain .15c today!'\n",
        "text2 = 'Sell APPL stocks now!'\n",
        "textsl = [text2]\n",
        "tokens = tokenizer.texts_to_sequences(textsl)\n",
        "tokens_pad = pad_sequences(tokens, maxlen = max_len)\n",
        "prediction = model.predict_classes(tokens_pad)\n",
        "prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "1v20-x66nxoF",
        "colab_type": "code",
        "outputId": "b529960a-fa76-4b40-8b46-fd8b39f05a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        }
      },
      "cell_type": "code",
      "source": [
        "#how to get the indexes of all the true negative values\n",
        "negatively = df.loc[df['sentiment'] == 'negative']\n",
        "values = negatively.index.tolist()\n",
        "values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6,\n",
              " 7,\n",
              " 21,\n",
              " 26,\n",
              " 31,\n",
              " 34,\n",
              " 38,\n",
              " 52,\n",
              " 54,\n",
              " 58,\n",
              " 59,\n",
              " 87,\n",
              " 97,\n",
              " 102,\n",
              " 104,\n",
              " 107,\n",
              " 109,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 119,\n",
              " 120,\n",
              " 124,\n",
              " 135,\n",
              " 137,\n",
              " 143,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 161,\n",
              " 164,\n",
              " 165,\n",
              " 173,\n",
              " 176,\n",
              " 178,\n",
              " 180,\n",
              " 181,\n",
              " 183,\n",
              " 185,\n",
              " 186,\n",
              " 188,\n",
              " 200,\n",
              " 202,\n",
              " 204,\n",
              " 206,\n",
              " 212,\n",
              " 227,\n",
              " 232,\n",
              " 238,\n",
              " 239,\n",
              " 246,\n",
              " 250,\n",
              " 259,\n",
              " 263,\n",
              " 268,\n",
              " 272,\n",
              " 274,\n",
              " 277,\n",
              " 282,\n",
              " 286,\n",
              " 287,\n",
              " 288,\n",
              " 289,\n",
              " 290,\n",
              " 291,\n",
              " 294,\n",
              " 295,\n",
              " 297,\n",
              " 300,\n",
              " 302,\n",
              " 309,\n",
              " 322,\n",
              " 323,\n",
              " 324,\n",
              " 325,\n",
              " 330,\n",
              " 335,\n",
              " 336,\n",
              " 338,\n",
              " 339,\n",
              " 340,\n",
              " 341,\n",
              " 344,\n",
              " 345,\n",
              " 346,\n",
              " 349,\n",
              " 350,\n",
              " 353,\n",
              " 355,\n",
              " 359,\n",
              " 361,\n",
              " 362,\n",
              " 365,\n",
              " 366,\n",
              " 368,\n",
              " 369,\n",
              " 370,\n",
              " 374,\n",
              " 378,\n",
              " 383,\n",
              " 385,\n",
              " 389,\n",
              " 395,\n",
              " 396,\n",
              " 397,\n",
              " 399,\n",
              " 400,\n",
              " 402,\n",
              " 404,\n",
              " 413,\n",
              " 414,\n",
              " 420,\n",
              " 422,\n",
              " 425,\n",
              " 427,\n",
              " 430,\n",
              " 431,\n",
              " 432,\n",
              " 433,\n",
              " 436,\n",
              " 438,\n",
              " 439,\n",
              " 442,\n",
              " 443,\n",
              " 448,\n",
              " 449,\n",
              " 453,\n",
              " 455,\n",
              " 456,\n",
              " 461,\n",
              " 462,\n",
              " 464,\n",
              " 471,\n",
              " 472,\n",
              " 474,\n",
              " 475,\n",
              " 476,\n",
              " 483,\n",
              " 486,\n",
              " 488,\n",
              " 490,\n",
              " 492,\n",
              " 493,\n",
              " 494,\n",
              " 496,\n",
              " 497,\n",
              " 498,\n",
              " 502,\n",
              " 503,\n",
              " 505,\n",
              " 506,\n",
              " 516,\n",
              " 517,\n",
              " 518,\n",
              " 523,\n",
              " 525,\n",
              " 527,\n",
              " 528,\n",
              " 529,\n",
              " 530,\n",
              " 534,\n",
              " 536,\n",
              " 537,\n",
              " 538,\n",
              " 541,\n",
              " 543,\n",
              " 552,\n",
              " 553,\n",
              " 557,\n",
              " 558,\n",
              " 562,\n",
              " 566,\n",
              " 569,\n",
              " 573,\n",
              " 584,\n",
              " 586,\n",
              " 593,\n",
              " 596,\n",
              " 598,\n",
              " 600,\n",
              " 602,\n",
              " 603,\n",
              " 604,\n",
              " 606,\n",
              " 607,\n",
              " 610,\n",
              " 611,\n",
              " 615,\n",
              " 626,\n",
              " 627,\n",
              " 632,\n",
              " 634,\n",
              " 637,\n",
              " 638,\n",
              " 643,\n",
              " 644,\n",
              " 646,\n",
              " 647,\n",
              " 649,\n",
              " 650,\n",
              " 653,\n",
              " 654,\n",
              " 656,\n",
              " 657,\n",
              " 659,\n",
              " 660,\n",
              " 662,\n",
              " 663,\n",
              " 664,\n",
              " 666,\n",
              " 667,\n",
              " 668,\n",
              " 669,\n",
              " 674,\n",
              " 676,\n",
              " 678,\n",
              " 679,\n",
              " 682,\n",
              " 689,\n",
              " 693,\n",
              " 696,\n",
              " 698,\n",
              " 700,\n",
              " 701,\n",
              " 705,\n",
              " 707,\n",
              " 708,\n",
              " 710,\n",
              " 712,\n",
              " 717,\n",
              " 720,\n",
              " 724,\n",
              " 728,\n",
              " 729,\n",
              " 730,\n",
              " 731,\n",
              " 732,\n",
              " 733,\n",
              " 736,\n",
              " 737,\n",
              " 738,\n",
              " 742,\n",
              " 750,\n",
              " 752,\n",
              " 755,\n",
              " 757,\n",
              " 758,\n",
              " 769,\n",
              " 770,\n",
              " 777,\n",
              " 783,\n",
              " 787,\n",
              " 788,\n",
              " 793,\n",
              " 796,\n",
              " 797,\n",
              " 800,\n",
              " 801,\n",
              " 807,\n",
              " 809,\n",
              " 811,\n",
              " 812,\n",
              " 813,\n",
              " 815,\n",
              " 816,\n",
              " 817,\n",
              " 822,\n",
              " 828,\n",
              " 838,\n",
              " 842,\n",
              " 844,\n",
              " 858,\n",
              " 860,\n",
              " 863,\n",
              " 866,\n",
              " 871,\n",
              " 872,\n",
              " 875,\n",
              " 879,\n",
              " 885,\n",
              " 892,\n",
              " 901,\n",
              " 902,\n",
              " 910,\n",
              " 911,\n",
              " 914,\n",
              " 921,\n",
              " 922,\n",
              " 926,\n",
              " 928,\n",
              " 931,\n",
              " 932,\n",
              " 947,\n",
              " 948,\n",
              " 960,\n",
              " 964,\n",
              " 966,\n",
              " 968,\n",
              " 969,\n",
              " 971,\n",
              " 976,\n",
              " 980,\n",
              " 987,\n",
              " 989,\n",
              " 990,\n",
              " 993,\n",
              " 995,\n",
              " 996,\n",
              " 997,\n",
              " 999,\n",
              " 1005,\n",
              " 1008,\n",
              " 1009,\n",
              " 1010,\n",
              " 1018,\n",
              " 1030,\n",
              " 1032,\n",
              " 1033,\n",
              " 1038,\n",
              " 1042,\n",
              " 1044,\n",
              " 1046,\n",
              " 1047,\n",
              " 1050,\n",
              " 1053,\n",
              " 1060,\n",
              " 1061,\n",
              " 1071,\n",
              " 1073,\n",
              " 1074,\n",
              " 1076,\n",
              " 1079,\n",
              " 1080,\n",
              " 1087,\n",
              " 1090,\n",
              " 1092,\n",
              " 1095,\n",
              " 1099,\n",
              " 1106,\n",
              " 1108,\n",
              " 1116,\n",
              " 1117,\n",
              " 1120,\n",
              " 1121,\n",
              " 1122,\n",
              " 1123,\n",
              " 1128,\n",
              " 1131,\n",
              " 1132,\n",
              " 1134,\n",
              " 1135,\n",
              " 1139,\n",
              " 1140,\n",
              " 1141,\n",
              " 1144,\n",
              " 1146,\n",
              " 1148,\n",
              " 1152,\n",
              " 1158,\n",
              " 1162,\n",
              " 1163,\n",
              " 1164,\n",
              " 1165,\n",
              " 1166,\n",
              " 1167,\n",
              " 1168,\n",
              " 1169,\n",
              " 1170,\n",
              " 1179,\n",
              " 1181,\n",
              " 1184,\n",
              " 1185,\n",
              " 1186,\n",
              " 1187,\n",
              " 1188,\n",
              " 1189,\n",
              " 1190,\n",
              " 1191,\n",
              " 1192,\n",
              " 1193,\n",
              " 1195,\n",
              " 1196,\n",
              " 1199,\n",
              " 1200,\n",
              " 1201,\n",
              " 1202,\n",
              " 1205,\n",
              " 1206,\n",
              " 1207,\n",
              " 1208,\n",
              " 1209,\n",
              " 1214,\n",
              " 1215,\n",
              " 1216,\n",
              " 1218,\n",
              " 1220,\n",
              " 1223,\n",
              " 1225,\n",
              " 1226,\n",
              " 1227,\n",
              " 1228,\n",
              " 1229,\n",
              " 1230,\n",
              " 1232,\n",
              " 1233,\n",
              " 1234,\n",
              " 1235,\n",
              " 1236,\n",
              " 1239,\n",
              " 1240,\n",
              " 1242,\n",
              " 1243,\n",
              " 1245,\n",
              " 1250,\n",
              " 1251,\n",
              " 1257,\n",
              " 1258,\n",
              " 1262,\n",
              " 1263,\n",
              " 1269,\n",
              " 1271,\n",
              " 1275,\n",
              " 1276,\n",
              " 1280,\n",
              " 1282,\n",
              " 1287,\n",
              " 1292,\n",
              " 1293,\n",
              " 1296,\n",
              " 1297,\n",
              " 1310,\n",
              " 1311,\n",
              " 1312,\n",
              " 1313,\n",
              " 1314,\n",
              " 1315,\n",
              " 1317,\n",
              " 1320,\n",
              " 1325,\n",
              " 1328,\n",
              " 1333,\n",
              " 1343,\n",
              " 1352,\n",
              " 1355,\n",
              " 1360,\n",
              " 1363,\n",
              " 1364,\n",
              " 1368,\n",
              " 1371,\n",
              " 1375,\n",
              " 1376,\n",
              " 1377,\n",
              " 1378,\n",
              " 1380,\n",
              " 1382,\n",
              " 1388,\n",
              " 1389,\n",
              " 1391,\n",
              " 1392,\n",
              " 1395,\n",
              " 1396,\n",
              " 1398,\n",
              " 1400,\n",
              " 1401,\n",
              " 1406,\n",
              " 1407,\n",
              " 1408,\n",
              " 1409,\n",
              " 1410,\n",
              " 1411,\n",
              " 1415,\n",
              " 1416,\n",
              " 1420,\n",
              " 1422,\n",
              " 1424,\n",
              " 1427,\n",
              " 1428,\n",
              " 1429,\n",
              " 1430,\n",
              " 1431,\n",
              " 1435,\n",
              " 1440,\n",
              " 1441,\n",
              " 1444,\n",
              " 1450,\n",
              " 1457,\n",
              " 1458,\n",
              " 1461,\n",
              " 1465,\n",
              " 1466,\n",
              " 1469,\n",
              " 1471,\n",
              " 1473,\n",
              " 1475,\n",
              " 1478,\n",
              " 1481,\n",
              " 1485,\n",
              " 1489,\n",
              " 1491,\n",
              " 1492,\n",
              " 1495,\n",
              " 1496,\n",
              " 1497,\n",
              " 1498,\n",
              " 1499,\n",
              " 1500,\n",
              " 1502,\n",
              " 1503,\n",
              " 1504,\n",
              " 1508,\n",
              " 1509,\n",
              " 1511,\n",
              " 1513,\n",
              " 1514,\n",
              " 1515,\n",
              " 1516,\n",
              " 1517,\n",
              " 1518,\n",
              " 1519,\n",
              " 1520,\n",
              " 1523,\n",
              " 1526,\n",
              " 1528,\n",
              " 1532,\n",
              " 1533,\n",
              " 1534,\n",
              " 1536,\n",
              " 1537,\n",
              " 1541,\n",
              " 1544,\n",
              " 1547,\n",
              " 1548,\n",
              " 1550,\n",
              " 1553,\n",
              " 1573,\n",
              " 1574,\n",
              " 1578,\n",
              " 1580,\n",
              " 1584,\n",
              " 1585,\n",
              " 1590,\n",
              " 1592,\n",
              " 1593,\n",
              " 1594,\n",
              " 1595,\n",
              " 1598,\n",
              " 1602,\n",
              " 1609,\n",
              " 1611,\n",
              " 1613,\n",
              " 1617,\n",
              " 1618,\n",
              " 1619,\n",
              " 1620,\n",
              " 1621,\n",
              " 1622,\n",
              " 1623,\n",
              " 1625,\n",
              " 1626,\n",
              " 1629,\n",
              " 1630,\n",
              " 1632,\n",
              " 1635,\n",
              " 1642,\n",
              " 1644,\n",
              " 1645,\n",
              " 1647,\n",
              " 1648,\n",
              " 1649,\n",
              " 1650,\n",
              " 1653,\n",
              " 1656,\n",
              " 1658,\n",
              " 1662,\n",
              " 1669,\n",
              " 1670,\n",
              " 1671,\n",
              " 1673,\n",
              " 1674,\n",
              " 1675,\n",
              " 1676,\n",
              " 1677,\n",
              " 1679,\n",
              " 1686,\n",
              " 1687,\n",
              " 1688,\n",
              " 1691,\n",
              " 1699,\n",
              " 1700,\n",
              " 1703,\n",
              " 1704,\n",
              " 1707,\n",
              " 1712,\n",
              " 1717,\n",
              " 1718,\n",
              " 1719,\n",
              " 1721,\n",
              " 1722,\n",
              " 1723,\n",
              " 1725,\n",
              " 1733,\n",
              " 1734,\n",
              " 1735,\n",
              " 1740,\n",
              " 1741,\n",
              " 1742,\n",
              " 1745,\n",
              " 1748,\n",
              " 1749,\n",
              " 1754,\n",
              " 1755,\n",
              " 1756,\n",
              " 1759,\n",
              " 1762,\n",
              " 1766,\n",
              " 1767,\n",
              " 1772,\n",
              " 1774,\n",
              " 1776,\n",
              " 1777,\n",
              " 1779,\n",
              " 1783,\n",
              " 1784,\n",
              " 1787,\n",
              " 1788,\n",
              " 1790,\n",
              " 1791,\n",
              " 1793,\n",
              " 1795,\n",
              " 1799,\n",
              " 1800,\n",
              " 1801,\n",
              " 1808,\n",
              " 1809,\n",
              " 1814,\n",
              " 1816,\n",
              " 1817,\n",
              " 1822,\n",
              " 1823,\n",
              " 1825,\n",
              " 1835,\n",
              " 1842,\n",
              " 1846,\n",
              " 1848,\n",
              " 1851,\n",
              " 1854,\n",
              " 1863,\n",
              " 1864,\n",
              " 1865,\n",
              " 1867,\n",
              " 1870,\n",
              " 1872,\n",
              " 1894,\n",
              " 1900,\n",
              " 1904,\n",
              " 1906,\n",
              " 1907,\n",
              " 1909,\n",
              " 1911,\n",
              " 1917,\n",
              " 1919,\n",
              " 1924,\n",
              " 1926,\n",
              " 1933,\n",
              " 1935,\n",
              " 1936,\n",
              " 1937,\n",
              " 1938,\n",
              " 1939,\n",
              " 1940,\n",
              " 1942,\n",
              " 1945,\n",
              " 1954,\n",
              " 1960,\n",
              " 1961,\n",
              " 1962,\n",
              " 1964,\n",
              " 1965,\n",
              " 1969,\n",
              " 1974,\n",
              " 1976,\n",
              " 1987,\n",
              " 1988,\n",
              " 1990,\n",
              " 1991,\n",
              " 1992,\n",
              " 1995,\n",
              " 1996,\n",
              " 1998,\n",
              " 1999,\n",
              " 2000,\n",
              " 2004,\n",
              " 2007,\n",
              " 2008,\n",
              " 2009,\n",
              " 2011,\n",
              " 2013,\n",
              " 2014,\n",
              " 2015,\n",
              " 2017,\n",
              " 2018,\n",
              " 2022,\n",
              " 2028,\n",
              " 2038,\n",
              " 2046,\n",
              " 2049,\n",
              " 2051,\n",
              " 2053,\n",
              " 2054,\n",
              " 2055,\n",
              " 2057,\n",
              " 2058,\n",
              " 2061,\n",
              " 2065,\n",
              " 2066,\n",
              " 2067,\n",
              " 2069,\n",
              " 2071,\n",
              " 2073,\n",
              " 2076,\n",
              " 2088,\n",
              " 2090,\n",
              " 2091,\n",
              " 2093,\n",
              " 2105,\n",
              " 2114,\n",
              " 2116,\n",
              " 2123,\n",
              " 2126,\n",
              " 2130,\n",
              " 2133,\n",
              " 2134,\n",
              " 2143,\n",
              " 2144,\n",
              " 2147,\n",
              " 2149,\n",
              " 2153,\n",
              " 2155,\n",
              " 2156,\n",
              " 2157,\n",
              " 2158,\n",
              " 2164,\n",
              " 2166,\n",
              " 2167,\n",
              " 2172,\n",
              " 2174,\n",
              " 2180,\n",
              " 2203,\n",
              " 2204,\n",
              " 2205,\n",
              " 2209,\n",
              " 2214,\n",
              " 2219,\n",
              " 2220,\n",
              " 2227,\n",
              " 2246,\n",
              " 2249,\n",
              " 2252,\n",
              " 2255,\n",
              " 2256,\n",
              " 2258,\n",
              " 2261,\n",
              " 2262,\n",
              " 2264,\n",
              " 2265,\n",
              " 2266,\n",
              " 2267,\n",
              " 2268,\n",
              " 2272,\n",
              " 2275,\n",
              " 2289,\n",
              " 2291,\n",
              " 2299,\n",
              " 2305,\n",
              " 2306,\n",
              " 2308,\n",
              " 2311,\n",
              " 2314,\n",
              " 2315,\n",
              " 2317,\n",
              " 2318,\n",
              " 2319,\n",
              " 2320,\n",
              " 2325,\n",
              " 2326,\n",
              " 2327,\n",
              " 2329,\n",
              " 2331,\n",
              " 2333,\n",
              " 2337,\n",
              " 2346,\n",
              " 2358,\n",
              " 2360,\n",
              " 2361,\n",
              " 2362,\n",
              " 2363,\n",
              " 2364,\n",
              " 2365,\n",
              " 2366,\n",
              " 2368,\n",
              " 2369,\n",
              " 2384,\n",
              " 2385,\n",
              " 2388,\n",
              " 2396,\n",
              " 2397,\n",
              " 2401,\n",
              " 2402,\n",
              " 2403,\n",
              " 2406,\n",
              " 2407,\n",
              " 2412,\n",
              " 2418,\n",
              " 2419,\n",
              " 2420,\n",
              " 2421,\n",
              " 2422,\n",
              " 2423,\n",
              " 2424,\n",
              " 2425,\n",
              " 2426,\n",
              " 2427,\n",
              " 2428,\n",
              " 2431,\n",
              " 2434,\n",
              " 2437,\n",
              " 2439,\n",
              " 2442,\n",
              " 2443,\n",
              " 2445,\n",
              " 2447,\n",
              " 2449,\n",
              " 2450,\n",
              " 2453,\n",
              " 2455,\n",
              " 2459,\n",
              " 2471,\n",
              " 2472,\n",
              " 2474,\n",
              " 2476,\n",
              " 2486,\n",
              " 2490,\n",
              " 2491,\n",
              " 2493,\n",
              " 2498,\n",
              " 2503,\n",
              " 2509,\n",
              " 2511,\n",
              " 2513,\n",
              " 2515,\n",
              " 2517,\n",
              " 2518,\n",
              " 2519,\n",
              " 2523,\n",
              " 2528,\n",
              " 2530,\n",
              " 2533,\n",
              " 2534,\n",
              " 2535,\n",
              " 2539,\n",
              " 2544,\n",
              " 2549,\n",
              " 2552,\n",
              " 2554,\n",
              " 2555,\n",
              " 2560,\n",
              " 2561,\n",
              " 2563,\n",
              " 2565,\n",
              " 2572,\n",
              " 2573,\n",
              " 2575,\n",
              " 2577,\n",
              " 2578,\n",
              " 2579,\n",
              " 2583,\n",
              " 2585,\n",
              " 2586,\n",
              " 2590,\n",
              " 2592,\n",
              " 2604,\n",
              " 2612,\n",
              " 2614,\n",
              " 2618,\n",
              " 2621,\n",
              " 2622,\n",
              " 2623,\n",
              " 2624,\n",
              " 2626,\n",
              " 2627,\n",
              " 2633,\n",
              " 2638,\n",
              " 2645,\n",
              " 2648,\n",
              " 2652,\n",
              " 2653,\n",
              " 2656,\n",
              " 2657,\n",
              " 2659,\n",
              " 2660,\n",
              " 2663,\n",
              " 2665,\n",
              " 2668,\n",
              " 2672,\n",
              " 2676,\n",
              " 2677,\n",
              " 2682,\n",
              " 2684,\n",
              " 2685,\n",
              " 2686,\n",
              " 2692,\n",
              " 2710,\n",
              " 2718,\n",
              " 2719,\n",
              " 2721,\n",
              " 2723,\n",
              " 2726,\n",
              " 2730,\n",
              " 2734,\n",
              " 2736,\n",
              " 2737,\n",
              " 2738,\n",
              " 2742,\n",
              " 2750,\n",
              " 2754,\n",
              " 2755,\n",
              " 2756,\n",
              " 2758,\n",
              " 2759,\n",
              " 2760,\n",
              " 2766,\n",
              " 2767,\n",
              " 2770,\n",
              " 2775,\n",
              " 2777,\n",
              " 2778,\n",
              " 2779,\n",
              " 2780,\n",
              " 2782,\n",
              " 2783,\n",
              " 2784,\n",
              " 2786,\n",
              " 2787,\n",
              " 2788,\n",
              " 2790,\n",
              " 2791,\n",
              " 2792,\n",
              " 2794,\n",
              " 2795,\n",
              " 2796,\n",
              " 2797,\n",
              " 2798,\n",
              " 2802,\n",
              " 2803,\n",
              " 2804,\n",
              " 2805,\n",
              " 2806,\n",
              " 2809,\n",
              " 2810,\n",
              " 2811,\n",
              " 2812,\n",
              " 2814,\n",
              " 2826,\n",
              " 2828,\n",
              " 2829,\n",
              " 2830,\n",
              " 2831,\n",
              " 2836,\n",
              " 2837,\n",
              " 2838,\n",
              " 2839,\n",
              " 2840,\n",
              " 2842,\n",
              " 2844,\n",
              " 2845,\n",
              " 2848,\n",
              " 2850,\n",
              " 2851,\n",
              " 2856,\n",
              " 2858,\n",
              " 2859,\n",
              " 2860,\n",
              " 2865,\n",
              " 2866,\n",
              " 2875,\n",
              " 2883,\n",
              " 2884,\n",
              " 2893,\n",
              " 2896,\n",
              " 2899,\n",
              " 2903,\n",
              " 2904,\n",
              " 2906,\n",
              " 2907,\n",
              " 2908,\n",
              " 2910,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "NgcQUckXojKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating a new dataframe with just those negative values\n",
        "negative_df = df.iloc[values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghbrSsnXp0b0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "negative_text = negative_df['text'].values #this returns just the negative text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQvlcPD7qHE5",
        "colab_type": "code",
        "outputId": "4865a298-c32d-4546-d822-81249f828d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "# we will test on the first 100 negative values to see if they are classified correctly.\n",
        "# this is not a valid form of testing because most of the values have already been tested and trainied on..\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences(negative_text[0:100])\n",
        "tokens_pad = pad_sequences(tokens, maxlen = max_len)\n",
        "prediction = model.predict_classes(tokens_pad)\n",
        "print(prediction)\n",
        "# false = np.where(prediction > .5)\n",
        "# false"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FOfUoDq5rGQ-",
        "colab_type": "code",
        "outputId": "6d2aa69b-13fa-4962-c3e4-9f6944bdd0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "negative_text[12]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$AIG American International Group Option Traders bet on 4% down move by next Friday URL '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "SdxFtMr_q1sW",
        "colab_type": "code",
        "outputId": "6f998988-1188-466b-8822-b7103c55b7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "falsely_classified = negative_text[false[0]]\n",
        "print(falsely_classified[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$LULU red, not ready for break out.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YYqvr7UOr25g",
        "colab_type": "code",
        "outputId": "96272dae-dbf7-49bb-c6a0-e4a60a7894c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "cell_type": "code",
      "source": [
        "#this is testing the sentiment analysis\n",
        "positive = df.loc[df['sentiment'] == 'positive']\n",
        "positive_text = positive['text'].values\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences(positive_text[100:200])\n",
        "tokens_pad = pad_sequences(tokens, maxlen = max_len)\n",
        "pre = model.predict(tokens_pad)\n",
        "#predict_classes shows the output either a 0 or 1 making it much easier to intrepret\n",
        "pre = model.predict_classes(tokens_pad)\n",
        "print(pre)\n",
        "false = np.where(prediction > .5)\n",
        "false"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9848696 ]\n",
            " [0.86262435]\n",
            " [0.98121625]\n",
            " [0.98884284]\n",
            " [0.07823612]\n",
            " [0.98560405]\n",
            " [0.9827214 ]\n",
            " [0.98902285]\n",
            " [0.97623116]\n",
            " [0.9893722 ]\n",
            " [0.98872584]\n",
            " [0.9781983 ]\n",
            " [0.9881813 ]\n",
            " [0.9683452 ]\n",
            " [0.99039453]\n",
            " [0.3597659 ]\n",
            " [0.9894567 ]\n",
            " [0.9867395 ]\n",
            " [0.9754174 ]\n",
            " [0.9892541 ]\n",
            " [0.983868  ]\n",
            " [0.9845463 ]\n",
            " [0.98686975]\n",
            " [0.9881045 ]\n",
            " [0.98976773]\n",
            " [0.08143286]\n",
            " [0.99023485]\n",
            " [0.98980707]\n",
            " [0.96539026]\n",
            " [0.95243573]\n",
            " [0.08189831]\n",
            " [0.98739296]\n",
            " [0.98809475]\n",
            " [0.983214  ]\n",
            " [0.9877029 ]\n",
            " [0.9847231 ]\n",
            " [0.9881516 ]\n",
            " [0.07540542]\n",
            " [0.93922687]\n",
            " [0.98620564]\n",
            " [0.9713596 ]\n",
            " [0.9841695 ]\n",
            " [0.98925215]\n",
            " [0.980911  ]\n",
            " [0.9877436 ]\n",
            " [0.9817535 ]\n",
            " [0.09801843]\n",
            " [0.9718165 ]\n",
            " [0.97959137]\n",
            " [0.97582936]\n",
            " [0.98114544]\n",
            " [0.9845255 ]\n",
            " [0.9856894 ]\n",
            " [0.9844387 ]\n",
            " [0.98794353]\n",
            " [0.97958714]\n",
            " [0.9862389 ]\n",
            " [0.06488881]\n",
            " [0.97880965]\n",
            " [0.8945298 ]\n",
            " [0.8124025 ]\n",
            " [0.98696274]\n",
            " [0.9868318 ]\n",
            " [0.9822174 ]\n",
            " [0.9867345 ]\n",
            " [0.9871897 ]\n",
            " [0.99065524]\n",
            " [0.98490477]\n",
            " [0.9758011 ]\n",
            " [0.98862517]\n",
            " [0.9609329 ]\n",
            " [0.9750183 ]\n",
            " [0.98200834]\n",
            " [0.98544955]\n",
            " [0.9841905 ]\n",
            " [0.9865769 ]\n",
            " [0.9397723 ]\n",
            " [0.9818286 ]\n",
            " [0.97873276]\n",
            " [0.98938555]\n",
            " [0.990157  ]\n",
            " [0.98378974]\n",
            " [0.98782736]\n",
            " [0.98806494]\n",
            " [0.98887897]\n",
            " [0.98909086]\n",
            " [0.99143714]\n",
            " [0.98642504]\n",
            " [0.97871727]\n",
            " [0.98615384]\n",
            " [0.98867536]\n",
            " [0.9538849 ]\n",
            " [0.8812759 ]\n",
            " [0.72176754]\n",
            " [0.9888016 ]\n",
            " [0.98162526]\n",
            " [0.98987514]\n",
            " [0.98308045]\n",
            " [0.98727417]\n",
            " [0.9888138 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([18, 24, 28, 31, 34, 40, 45, 49, 52, 53, 56, 66, 67, 71, 80, 83, 97]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "PWJMtqzcs5qC",
        "colab_type": "code",
        "outputId": "297eaf4a-4426-44b6-c3be-9d4a98ec3392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "falsely_classified = positive_text[false[0]]\n",
        "print(len(falsely_classified)) # only 6 misclassifications in the positive test\n",
        "print(falsely_classified[:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "['Over extended stock market + unexpected news from the FED minutes = Down $EURUSD & a panicky $ES_F $NQ_F.'\n",
            " '$BAC what happens next? URL '\n",
            " '$AAPL moving down with volume. Couldnt cope with the descending trend line. Trend is still down. Perhaps 460 is not impossible after all.'\n",
            " '$AAPL people be realistic. Market undecided what to do with apple. Earnings will lift that cloud. If GOOD we might REALLY see a reversal.'\n",
            " \"$AMZN alert update: a last minute push tried to shake out puts. we're hanging on for now. URL \"\n",
            " '$HMA unusual option activity. Trader buys 11k of the Jan 9 Put. Short term bear play'\n",
            " '$ZNGA Still shows red and dead on a Renko view, PF Box size 0.25 URL '\n",
            " '@user $BAC Yeah ... they made the easy profits by slashing jobs, now they have to close win-win sales.'\n",
            " '$ko has trouble staying above 200 day sma past few weeks. could short here or higher stops above 38 URL '\n",
            " '$mako a red day monday breaks support and could test old lows of 9-9.25 URL '\n",
            " 'I think $AAPL is heading back down to check in on 506 support again'\n",
            " '$AA option traders bet on bad earnings selling 6,200 Jan 9C and buying 3,200 Jan 9P. Could be hedge on stock. Both trades against low OI'\n",
            " '$SKX Q3 operational cash flow negative $60million. $80million decline vs.Q3 2011'\n",
            " '3 horsemen of high beta momo chase $FSLR $GMCR $NFLX being slammed a bit here : advanced tell that the new year party is over for the mrkt ?'\n",
            " '$yum down premarket through the cloud but has support at about 62.75  URL '\n",
            " '$AYI Q1 Operational Cash Flow turns Negative'\n",
            " 'Let the angry, anecdotal comments begin: \"Why I still think BlackBerry & RIM are doomed.\" URL $RIMM $AAPL $GOOG $NOK $T $VZ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "99xkX93gtk8M",
        "colab_type": "code",
        "outputId": "6c7036c3-70f8-45b2-f05a-93371a7d0675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "print(positive_text[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['$AAPL - @user if so then the current downtrend will break. Otherwise just a short-term correction in med-term downtrend.'\n",
            " \"Monday's relative weakness. $NYX $WIN $TIE $TAP $ICE $INTU $BMC $AON $CL $CHK $BIIB URL \"\n",
            " \"Won't believe $AAPL uptrend is back until it crosses above MA(50)\"\n",
            " '$LULU red, not ready for break out.'\n",
            " '\"@user: been adding $UVXY long off the bottom today for trade, also got $WPI near low\"'\n",
            " '$LNKD looking like a good short. Failed to break price level resistance at 116 today.'\n",
            " 'Too early to short into this move. Stock Market needs a few days to settle down. #Patience URL $COH $BWLD $DLTR $AAPL $PAY'\n",
            " '$PHM PulteGroup Option Bear bets $1.5 Million on 11% down move by April URL '\n",
            " 'Short Setups displaying relative weakness: $ARO $COH $PAY $BWLD $DLTR $SHLD $WTW $UPL Prepare to short the stall. $ES_F URL '\n",
            " '$SKUL to the GRAVE!'\n",
            " '$ovti ttm ending Oct 2012 Negative $151 million operational cash flow.. 273% decline vs yoy ttm ending oct 2011'\n",
            " '$GPS wow that wa s a fast fast fade...'\n",
            " '$AIG American International Group Option Traders bet on 4% down move by next Friday URL '\n",
            " 'New Sony pre-owned block tech patent unearthed $SNE ... 50% of $GME profits from \"pre owned\" ..last one out turn off the lights'\n",
            " '\"@user: $SPX 1464.90 Wave iv would be 20 points lower [1467.00 KEY] $ES_F HOD 1460.00 (under neckline 1458.50 very bearish) \"'\n",
            " 'Wow, not good for $BKS RT @user ... sales for the Nook were down 13% over the holidays URL '\n",
            " 'The new wightwatchers ads are more fun and light and with regular people more interesting, but I still dont trust them $WTW (no position)'\n",
            " '$AAPL $SPY no more QE for 2013. Now bring the Geithner.'\n",
            " 'Over extended stock market + unexpected news from the FED minutes = Down $EURUSD & a panicky $ES_F $NQ_F.'\n",
            " '$SPY Uncle Ben needed to spice up things a little. $AAPL people were bored, wake up now.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ACpoei5AsYRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('stock_sentiment.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_PHjY4vqsoTU",
        "colab_type": "code",
        "outputId": "11321592-5c60-4a4e-b226-3aeaa312c14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'Stock_sentiment.h5'})\n",
        "uploaded.SetContentFile('stock_sentiment.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1kNbLFXYLrbnA0dBNftXz5cSmSAk7oRMo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "grEQihM6vIUG",
        "colab_type": "code",
        "outputId": "b956f618-13d5-4db4-e83b-5cb56385f878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_classes(x= X_test[:]) \n",
        "#this prints out the predictions in a more readible manner\n",
        "predictions = predictions.T[0]\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "VZt4OLGfw_Dy",
        "colab_type": "code",
        "outputId": "aa0d7e47-e1a2-4c0b-f358-391447f54057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "print(y_test[:].T)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1\n",
            "  1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1\n",
            "  1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            "  1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
            "  1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            "  1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            "  0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
            "  1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
            "  1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
            "  1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0\n",
            "  1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
            "  1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
            "  0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            "  0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            "  1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0\n",
            "  0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
            "  1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            "  1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
            "  1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0\n",
            "  1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            "  0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            "  0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
            "  1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            "  0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            "  1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0\n",
            "  1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0\n",
            "  1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ohlfMO9mwVeD",
        "colab_type": "code",
        "outputId": "ab9359c6-86e7-4266-b2ab-bf4005ad7646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "correct = np.array(y_test[:].T) #these are the labels associated with those test variables that we gather\n",
        "incorrect= np.where(predictions != correct)\n",
        "incorrect = incorrect[0]\n",
        "print(len(incorrect))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhB8HI_KzIdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''From these tests we learned a couple things\n",
        "predict_classes predicts the output without any probabilities involved\n",
        "We can check which are incorrect by comparing the predicted probabilities with the actual \n",
        "labels.\n",
        "The reason why the np.where clause was not executing the first time was because of the .T we transposed\n",
        "the results in order to make the legible, but disregarded the fact that we would be comparing them to \n",
        "another array which would have differing dimensions.'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}